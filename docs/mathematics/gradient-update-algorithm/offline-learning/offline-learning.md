# 梯度下降算法

* [返回上层目录](../gradient-update-algorithm.md)
* [梯度下降算法的演化](gradient-descent-algorithms-evolution/gradient-descent-algorithms-evolution.md)
* [随机梯度下降SGD](sgd/sgd.md)
* [动量法Momentum](momentum/momentum.md)
* [牛顿动量Nesterov](nesterov/nesterov.md)
* [AdaGrad](adagrad/adagrad.md)
* [RMSprop](rmsprop/rmsprop.md)
* [Adadelta](adadelta/adadelta.md)
* [Adam](adam/adam.md)
* [Nadam](nadam/nadam.md)
* [AMSGrad](amsgrad/amsgrad.md)
* [AdasMax](adamax/adamax.md)



# 参考资料

* [Deep Learning 之 最优化方法](https://blog.csdn.net/BVL10101111/article/details/72614711)

* [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)

* [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)

* [10个梯度下降优化算法+备忘单](https://ai.yanxishe.com/page/TextTranslation/1603?from=singlemessage)

  [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)

===

[优化方法总结以及Adam存在的问题(SGD, Momentum, AdaDelta, Adam, AdamW，LazyAdam)](https://blog.csdn.net/yinyu19950811/article/details/90476956)

